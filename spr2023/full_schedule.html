
<!DOCTYPE html>
<html lang="en">
<head>

    <meta charset="utf-8">
    <title>NENLP: New England NLP Meeting Series</title>
    <meta name="description" content="">
    <meta name="author" content="">

    <meta name="viewport" content="width=device-width, initial-scale=1">


    <link href="//fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">


    <link rel="stylesheet" href="css/normalize.css">
    <link rel="stylesheet" href="css/skeleton.css">
    <link rel="stylesheet" href="css/main.css">


</head>
<body>


<div class="container">
    <header class="row">
        <h3>New England NLP Meeting Series<br/>
       <img src="images/nenlp-logo-big.png" width=300 height=200>
        </h3>
 
    </header>

    <div class="row">
        <div class="three columns">


            <a class="button menu" href="index.html">Home</a>
            <a class="button menu" href="https://forms.gle/Zz6LGGKqvejERFrv5">Registration</a>
            <a class="button menu" href="full_schedule.html">Schedule</a>

<!-- Put the logos somewhere?          
            <p align=center>
            <img src="images/mit.png" width=60 height=60>
            <img src="images/umass_lowell.png" width=60 height=60>
            <img src="images/brown.jpeg" width=60 height=60>
            <img src="images/umass_amherst.png" width=60 height=60>
            <img src="images/brandeis.jpeg" width=60 height=60>
            <img src="images/northeastern.jpeg" width=60 height=60>
            <img src="images/bu.png" width=60 height=60>
            </p>
-->

        </div>

        <div class="nine columns">
            <!--<p style="color:red;font-weight:bold;font-size:20px">-->
            <!--Due to the weather conditions the invitational round at our site was moved to tomorrow (March 9) at the same time!-->
            <!--</p>-->

            <h5 align="center">NENLP 2023 Meetup Schedule</h5>
              
        <table>
          <tbody>
            <tr>
                <td>10:00-10:15</td>
                <td>Breakfast</td>
            </tr>
            <tr>
                <td>10:15-10:30</td>
                <td>Welcome remarks</td>
            </tr>
            <tr>
                <td valign="top" width="15%">10:30-11:50</td>
                <td><b>Oral talks</b>
                    <table cellspacing="0" cellpadding="0">
                        <tr>
                            <td>10:30-10:50</td>
                            <td>Afra Feyza Akyurek</td>
                            <td>Boston university</td>
                            <td>RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs</td>
                        </tr>
                        <tr>
                            <td>10:50-11:10</td>
                            <td>Evan Hernandez</td>
                            <td>MIT</td>
                            <td>Latent Linear Relational Embeddings in Transformer Language Models</td>
                        </tr>
                        <tr>
                            <td>11:10-11:30</td>
                            <td>Vlad Lialin</td>
                            <td>UMass Lowell</td>
                            <td>Parameter-efficient fine-tuning. A survey</td>
                        </tr>
                        <tr>
                            <td>11:30-11:50</td>
                            <td>Denis Jered McInerney</td>
                            <td>Northeastern University</td>
                            <td>CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical Notes with Large Language Models</td>
                        </tr>
                    </table>
                </td>
            </tr>
            <tr>
                <td>11:50-12:35</td>
                <td><b>Keynote 1: Shankar Ananthakrishan (Amazon Alexa AI)</b></td>
            </tr>
            <tr>
                <td>12:35-1:30</td>
                <td>Lunch</td>
            </tr>
            <tr>
                <td>1:30-3:00</td>
                <td><b>Poster session</b></td>
            </tr>
            <tr>
                <td>3:00-4:00</td>
                <td><b>Oral talks</b>
                    <table cellspacing="0" cellpadding="0">
                        <tr>
                            <td>3:00-3:20</td>
                            <td>Jack Merullo</td>
                            <td>Brown University</td>
                            <td>Feed Forward Networks Implement Abstract Functions that Transfer Across Contexts</td>
                            
                        </tr>
                        <tr>
                            <td>3:20-3:40</td>
                            <td>Varshini Subhash</td>
                            <td>Harvard University</td>
                            <td>Why do universal adversarial attacks work on large language models?</td>
                        </tr>
                        <tr>
                            <td>3:40-4:00</td>
                            <td>Erica Cai, Brendan O'Connor</td>
                            <td>UMass Amherst</td>
                            <td>Efficient and modality-independent zero-shot event extraction of entities with actor representatives</td>
                        </tr>
                    </table>
                </td>
            </tr>
            <tr>
                <td>4:00-4:45</td>
                <td><b>Keynote 2: Matthew McDermott (Harvard University)</b></td>
            </tr>
            <tr>
                <td>4:45-5:00</td>
                <td>Closing remarks</td>
            </tr>
        </table>
  


            <!--<tr id="Spotlight1">
              <td valign="top" width="15%">10:30-12:10</td>
              <td><b>Spotlight talks 1</b>

                <table cellspacing="0" cellpadding="0">
                <tr>
                    <td>10:30</td>
                    <td>Sarthak Jain</td>
                    <td>Northeastern University</td>
                    <td>Does BERT Pretrained on Clinical Notes Reveal Sensitive Data?</td>
                </tr>
                <tr><td>10:55</td>
                    <td>Zhichao Yang</td>
                    <td>UMass Lowell / UMass Amherst</td>
                    <td>Generating Accurate Electronic Health Assessment using Medical Knowledge Graph</td>
                </tr>
                <tr><td>11:20</td>
                    <td>Athul Paul Jacob</td>
                    <td>MIT</td>
                    <td>Modeling Strong and Human-like Gameplay with KL-Regularized Search</td>
                </tr>
                <tr><td>11:45</td>
                    <td>Pengshan Cai</td>
                    <td>UMass Amherst</td>
                    <td>Dialogue system reinforced for information acquisition</td>
                </tr>
                <tr><td>12:10</td>
                    <td>Vladislav Lialin</td>
                    <td>UMass Lowell / UMass Amherst</td>
                    <td>Life after BERT: What do Other Muppets Understand about Language?</td>
                </tr>

                </table>
              </td>

            </tr>
            <tr>
              <td>12:10-1:00</td>
              <td>Lunch</td>
            </tr>
            <tr>
              <td>1:00-2:30</td>
              <td><a href="#posters">Poster session</a></td>
            </tr>
            <tr id="Spotlight2">
              <td valign="top">2:30-4:10</td>
              <td><b>Spotlight talks 2</b>

                <table>
                <tr>
                    <td>2:30</td>
                    <td>Bhanu Rawat</td>
                    <td>UMass Amherst</td>
                    <td>Identifying Suicide Attempt and Ideation Events from EHRs</td>
                </tr>
                <tr>
                    <td>2:55</td>
                    <td>Albert Webson</td>
                    <td>Brown University</td>
                    <td>Do Prompt-Based Models Really Understand the Meaning of Their Prompts?</td>
                </tr>
                <tr>
                    <td>3:20</td>
                    <td>Denis McInerny</td>
                    <td></td>
                    <td>That’s the Wrong Lung!</td>
                </tr>
                <tr>
                    <td>3:45</td>
                    <td>Pratyusha Sharma</td>
                    <td>MIT</td>
                    <td>Skill induction and planning with latent language</td>
                </tr>
                </table>


              </td>
            </tr>
            <tr>
              <td>4:10-4:30</td>
              <td>Concluding remarks & discussion</td>
            </tr>
          </tbody>
        </table>-->

        <h5 id="posters">Poster session</h5>

        <table>
        <tr>
            <td>Sheridan Feucht</td>
            <td>Brown University</td>    
            <td>Can Visual Models Learn an Abstract Relation from Data?<td>
        </tr>
        <tr>
            <td>Catherine Chen</td>
            <td>Brown University</td>    
            <td>Evaluating Search Explainability with Psychometrics and Crowdsourcing<td>
        </tr>
        <tr>
            <td>Alex Gu</td>
            <td>MIT</td>
            <td>ObSynth: An Interactive Synthesis System for Generating Object Models from Natural Language Specifications</td>
        </tr>
        <tr>
            <td>Michal Golovanevsky</td>
            <td>Brown University</td>
            <td>Scalable and Interpretable Multimodal Attention</td>
        </tr>
        <tr>
            <td>Zhaofeng Wu</td>
            <td>MIT</td>
            <td>Transparency Helps Reveal When Language Models Learn Meaning</td>
        </tr>
        <tr>
            <td>Isidora Tourni</td>
            <td>Boston University</td>
            <td>An Empirical study of Unsupervised Neural Machine Translation: analyzing NMT output, model’s behavior and sentences’ contribution</td>
        </tr>
        <tr>
            <td>William Rudman</td>
            <td>Brown University</td>
            <td>Stable Anisotropic Regularization</td>
        </tr>
        <tr>
            <td>Aaron Traylor</td>
            <td>Brown University</td>
            <td>Analyzing Transformer Mechanisms for Cognitive Branching</td>
        </tr>
        <tr>
            <td>Qinan Yu, Alyssa Loo</td>
            <td>Brown University</td>
            <td>Are Language Models Worse than Humans at Following Prompts? It's Complicated</td>
        </tr>
        <tr>
            <td>Ankita Gupta</td>
            <td>UMass Amherst</td>
            <td>ezCoref: Towards Unifying Annotation Guidelines for Coreference Resolution</td>
        </tr>
        <tr>
            <td>Chau Pham and Marisa Hudspeth</td>
            <td>UMass Amherst</td>
            <td>Gender and Power in Latin Narratives</td>
        </tr>
        <tr>
            <td>Charles Lovering</td>
            <td>Brown University</td>
            <td>Training Priors Predict Text-To-Image Model Performance</td>
        </tr>
        <tr>
            <td>Afra Feyza Akyurek</td>
            <td>Boston university</td>
            <td>RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs</td>
        </tr>
        <tr>
            <td>Evan Hernandez</td>
            <td>MIT</td>
            <td>Latent Linear Relational Embeddings in Transformer Language Models</td>
        </tr>
        <tr>
            <td>Vlad Lialin</td>
            <td>UMass Lowell</td>
            <td>Parameter-efficient fine-tuning. A survey</td>
        </tr>
        <tr>
            <td>Denis Jered McInerney</td>
            <td>Northeastern University</td>
            <td>CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical Notes with Large Language Models</td>
        </tr>
        <tr>
            <td>Jack Merullo</td>
            <td>Brown University</td>
            <td>Feed Forward Networks Implement Abstract Functions that Transfer Across Contexts</td>
            
        </tr>
        <tr>
            <td>Varshini Subhash</td>
            <td>Harvard University</td>
            <td>Why do universal adversarial attacks work on large language models?</td>
        </tr>
        <tr>
            <td>Erica Cai, Brendan O'Connor</td>
            <td>UMass Amherst</td>
            <td>Efficient and modality-independent zero-shot event extraction of entities with actor representatives</td>
        </tr>


    </div>
</div>



</body>
</html>
